<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>图像浅学：卷积神经网络及其发展 - CNN | SLie's Blog|琴弦之轮</title><meta name="author" content="Slie"><meta name="copyright" content="Slie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文章将从原理开始，以时间为轴讲述CNN的发展，包括 LeNet、AlexNet、VGG、ResNet等."><link rel="shortcut icon" href="/img/%E7%A3%81%E5%B8%A6.png"><link rel="canonical" href="https://qslie.top/posts/a5df071/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/pluginsSrc/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!0,top_n_per_article:1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,noticeOutdate:{limitDay:90,position:"top",messagePrev:"本文章距离上次修改已经过了",messageNext:"天，相关内容可能不再适用，请留意."},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:200},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"mediumZoom",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"bottom-center"},source:{justifiedGallery:{js:"/pluginsSrc/flickr-justified-gallery/dist/fjGallery.min.js",css:"/pluginsSrc/flickr-justified-gallery/dist/fjGallery.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"图像浅学：卷积神经网络及其发展 - CNN",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-03-11 22:44:06"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise(((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,n)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=e,t&&(a.id=t),a.onerror=n,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,o())},document.head.appendChild(a)})),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="/css/fixed_comment.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="/css/twikoo_beautify.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2022/04/09/Likong.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">182</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">98</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>声影</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i> <span>相册</span></a></li><li><a class="site-page child" href="/films/"><i class="fa-fw fas fa-video"></i> <span>映像</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i> <span>图书</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i> <span>游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081327503.png')"><nav id="nav"><span id="blog-info"><a href="/" title="SLie's Blog|琴弦之轮"><span class="site-name">SLie's Blog|琴弦之轮</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i> <span>声影</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i> <span>相册</span></a></li><li><a class="site-page child" href="/films/"><i class="fa-fw fas fa-video"></i> <span>映像</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i> <span>图书</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i> <span>游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">图像浅学：卷积神经网络及其发展 - CNN</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-07T06:05:40.000Z" title="发表于 2023-10-07 14:05:40">2023-10-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-11T14:44:06.441Z" title="更新于 2024-03-11 22:44:06">2024-03-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/">计算机科学与技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">机器学习与深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="图像浅学：卷积神经网络及其发展 - CNN"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img gist" style="background-image:url(https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081327503.png)"></div><article class="post-content" id="article-container"><p><strong>卷积神经网络</strong>（<em>convolutional neural network</em>，<em><strong>CNN</strong></em>）是⼀类强⼤的、为处理图像数据⽽设计的神经⽹络。基于卷积神经⽹络架构的模型在计算机视觉领域中已经占主导地位，当今⼏乎所有的图像识别、⽬标检测或语义分割相关的学术竞赛和商业应⽤都以这种⽅法为基础。</p><p>本文将从原理出发对CNN进行简单的梳理，然后提供 PyTorch 版本的简单实现。</p><h2 id="为什么是卷积">为什么是卷积</h2><p>对于图像识别任务，一个合理的假设是：⽆论采用哪种⽅法找到这个物体，这个方法本身都应该和物体的位置⽆关——“<em>沃尔多的样⼦并不取决于他潜藏的地⽅</em>”。</p><p>更加学术的描述是这样的，我们希望图像识别的方法具有以下两种性质：</p><ol><li>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经⽹络的前⾯⼏层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li>局部性（locality）：神经⽹络的前⾯⼏层应该只探索输⼊图像中的局部区域，⽽不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。可以在最后聚合这些局部特征，以在整个图像级别进⾏预测。</li></ol><p>而 <strong>二维卷积，正是符合以上两种性质的不二之选！</strong></p><h3 id="从-FC-到-Conv2d">从 FC 到 Conv2d</h3><p>思考卷积的由来时，有这样一种思路：从全连接层的原始形式做改进，最后逐步修正为卷积操作。</p><p>对于传统的全连接层来说，拓展到二维上，输入数据<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 经过全连接层得到输出数据（隐藏表示）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span></span></span></span> ，它们的尺寸可以不同。比如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span></span></span></span> 位置在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 的元素<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[H]_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span> ，它的值由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 的所有元素<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>X</mi><msub><mo stretchy="false">]</mo><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[X]_{k,l}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span>通过加权<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>W</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[W]_{i,j,k,l}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span> 和偏置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>U</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[U]_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10903em">U</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span> 得到。 即：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><munder><mo>∑</mo><mi>l</mi></munder><mo stretchy="false">[</mo><mi>W</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow></msub><mo stretchy="false">[</mo><mi>X</mi><msub><mo stretchy="false">]</mo><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow></msub><mo>+</mo><mo stretchy="false">[</mo><mi>U</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[H]_{i,j}=\sum_k\sum_l [W]_{i,j,k,l}[X]_{k,l}+[U]_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8479em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8479em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.01968em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10903em">U</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span></span></p><p>相当于用和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 同样大小的矩阵与之对应元素相乘，最后全部累加起来再加上偏置，得到隐藏表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span></span></span></span> 在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 处的元素。</p><hr><p>上述公式还可以写成：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>a</mi></munder><munder><mo>∑</mo><mi>b</mi></munder><mo stretchy="false">[</mo><mi>V</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><mo stretchy="false">[</mo><mi>X</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo>+</mo><mi>a</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>b</mi></mrow></msub><mo>+</mo><mo stretchy="false">[</mo><mi>U</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[H]_{i,j}=\sum_a\sum_b [V]_{i,j,a,b}[X]_{i+a,j+b}+[U]_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.9em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8479em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10903em">U</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span></span></p><p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>V</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><mi>W</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>i</mi><mo>+</mo><mi>a</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>b</mi></mrow></msub><mo separator="true">,</mo><mspace width="1em"><mi>k</mi><mo>=</mo><mi>i</mi><mo>+</mo><mi>a</mi><mo separator="true">,</mo><mtext>  </mtext><mi>l</mi><mo>=</mo><mi>j</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">[V]_{i,j,a,b}=[W]_{i,j,i+a,j+b},\quad k=i+a,\;l=j+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.7429em;vertical-align:-.0833em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.2778em"></span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.01968em">l</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.854em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6944em"></span><span class="mord mathnormal">b</span></span></span></span>.</p><p>相当于对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span></span></span></span> 内的元素进行重排得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 的索引是以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 为中心的。这使得权重矩阵、输入和输出都聚焦于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 。</p><p>接下来考虑针对两种性质对全连接层做修改。</p><h4 id="平移不变性">平移不变性</h4><p>平移不变性意味着检测目标无论在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 中进行怎样的平移，也应该只有隐藏表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span></span></span></span> 会发生平移现象（数值不变）。换言之，检测目标在不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> ，与之对应的权重和偏置应该不能有变化，这才能使针对检测目标的输出响应不变。即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">U</span></span></span></span> 不随<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 发生改变。<br>于是有：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>a</mi></munder><munder><mo>∑</mo><mi>b</mi></munder><mo stretchy="false">[</mo><mi>V</mi><msub><mo stretchy="false">]</mo><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><mo stretchy="false">[</mo><mi>X</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo>+</mo><mi>a</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>b</mi></mrow></msub><mo>+</mo><mi>u</mi></mrow><annotation encoding="application/x-tex">[H]_{i,j}=\sum_a\sum_b [V]_{a,b}[X]_{i+a,j+b}+u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.9em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8479em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">u</span></span></span></span></span></p><h4 id="局部性">局部性</h4><p>为了收集⽤来训练参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">[H]_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span></span></span></span> 的相关信息，我们不应偏离到距<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">)</span></span></span></span> 很远的地⽅。这意味着在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>a</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo><mtext>∆</mtext></mrow><annotation encoding="application/x-tex">|a| &gt; ∆</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:0"></span><span class="mord">∆</span></span></span></span> 或<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>b</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo><mtext>∆</mtext></mrow><annotation encoding="application/x-tex">|b| &gt; ∆</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">∣</span><span class="mord mathnormal">b</span><span class="mord">∣</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:0"></span><span class="mord">∆</span></span></span></span> 的范围之外，我们置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>V</mi><msub><mo stretchy="false">]</mo><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">[V]_{a,b} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">0</span></span></span></span>。相当于累加时只是小范围相乘。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><mi>H</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mo>−</mo><mi mathvariant="normal">Δ</mi></mrow><mi mathvariant="normal">Δ</mi></munderover><munderover><mo>∑</mo><mrow><mi>b</mi><mo>=</mo><mo>−</mo><mi mathvariant="normal">Δ</mi></mrow><mi mathvariant="normal">Δ</mi></munderover><mo stretchy="false">[</mo><mi>V</mi><msub><mo stretchy="false">]</mo><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><mo stretchy="false">[</mo><mi>X</mi><msub><mo stretchy="false">]</mo><mrow><mi>i</mi><mo>+</mo><mi>a</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>b</mi></mrow></msub><mo>+</mo><mi>u</mi></mrow><annotation encoding="application/x-tex">[H]_{i,j}=\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta} [V]_{a,b}[X]_{i+a,j+b}+u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3117em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:3.1888em;vertical-align:-1.3604em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8557em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">=</span><span class="mord mtight">−</span><span class="mord mtight">Δ</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3527em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8479em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mrel mtight">=</span><span class="mord mtight">−</span><span class="mord mtight">Δ</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3604em"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">u</span></span></span></span></span></p><p>而这个公式正好就是二维离散“<strong>卷积</strong>”操作(如图所示)。换言之，因为二维卷积具有平移不变性和局部性这种优良特性，使得参数规模大幅度下降的同时也能很好地处理图像任务，所以它才如此流行。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071810713.gif" alt="计算示意图"></p><div class="note primary modern"><p>这两种特性也叫卷积神经网络的 <strong>局部连接</strong> 和 <strong>权重共享</strong></p></div><h3 id="互相关运算">互相关运算</h3><p>事实上，上述得到的公式其实二维数据<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 和<strong>模板</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 之间的<strong>互相关运算</strong>。（模板也就是前面提到的<strong>权重矩阵</strong>；数字图像处理中也叫模板算子、卷积核、滤波器，对应于空间滤波的概念）</p><p>互相关运算描述的就是将卷积核与图像进行<strong>滑动窗口的对应相乘并求和</strong>得到输出元素。<br>这与二维卷积有着一些区别，根据二维卷积的公式，<strong>真正的卷积</strong>操作在数值上应该等同于对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 进行<strong>转置</strong>（旋转180度）然后再和卷积核进行互相关运算。</p><p>至于为什么我们严格执行卷积操作，那是因为在深度学习训练过程中，我们学习的参数就是模板元素的取值，因此⽆论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。</p><p>为了与深度学习⽂献中的标准术语保持⼀致，我们将继续把“互相关运算”<strong>称为卷积运算</strong>，尽管严格地说，它们略有不同。</p><p>本站图像处理相关跳转文章：</p><div class="tag link"><a class="link-card" title="数字图像及其基础处理-Image Processing" href="https://qslie.top/posts/d91fc48d/"><div class="left"><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/iconCode.png"></div><div class="right"><p class="text">数字图像及其基础处理-Image Processing</p><p class="url">https://qslie.top/posts/d91fc48d/</p></div></a></div><h2 id="其他术语">其他术语</h2><h3 id="通道-Channel">通道 |Channel</h3><p>在前面的描述中，我们忽略了图像⼀般包含三个通道/三种原⾊（红⾊、绿⾊和蓝⾊）。实际上，图像不是⼆维张量，⽽是⼀个由⾼度、宽度和颜⾊组成的三维张量，⽐如包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1024</mn><mo>×</mo><mn>1024</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">1024 × 1024 × 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">1024</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">1024</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">3</span></span></span></span> 个像素。前两个轴与像素的空间位置有关，⽽第三个轴可以看作每个像素的多维表示。</p><p>对于每⼀个空间位置，我们想要<strong>采用⼀组而不是⼀个</strong>隐藏表示。这样⼀组隐藏表示可以想象成⼀些互相堆叠的⼆维⽹格。因此，我们可以把隐藏表示想象为<strong>⼀系列具有⼆维张量的通道（channel）</strong>。这些通道有时<strong>也被称为特征映射（feature maps）</strong>（也有译作：<strong>特征图</strong>），因为每个通道都向后续层提供⼀组空间化的学习特征。</p><p>因此，此前的公式中的卷积核应该针对RGB三色通道扩充维度，使得卷积核来到了<strong>三维</strong>。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071830447.png" alt="多输入通道的卷积示例"></p><p>此外，特征图的张数（输出通道数）也可以自定义扩张，针对每一个特征图（输出）学习不同的三维卷积核（针对多输入），最终总体来说卷积核也就扩张到了<strong>四维</strong>（长、宽、输入通道、输出通道）。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071837438.png" alt="多输入多输出通道的卷积示例"></p><h3 id="感受野-Receptive-field">感受野 |Receptive field</h3><p>在卷积神经⽹络中，对于某⼀层的任意元素<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>，其 <strong>感受野（receptive field）</strong> 是指在前向传播期间可能影响<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span> 计算的所有元素（来⾃所有先前层）。</p><h3 id="填充-Padding">填充 |Padding</h3><p>假设输⼊形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>h</mi></msub><mo>×</mo><msub><mi>n</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">n_h × n_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> ，卷积核形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>h</mi></msub><mo>×</mo><msub><mi>k</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">k_h × k_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0315em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0315em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> ，那么输出形状将是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>n</mi><mi>h</mi></msub><mo>−</mo><msub><mi>k</mi><mi>h</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>w</mi></msub><mo>−</mo><msub><mi>k</mi><mi>w</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_h − k_h + 1) × (n_w − k_w + 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.0315em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.8444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.0315em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>。<br>因此，卷积的输出形状取决于输⼊形状和卷积核的形状。在应⽤多层卷积时，我们因此常常<strong>丢失边缘像素</strong>。</p><p>尽管我们通常使⽤较小的卷积核，因此对于任何单个卷积，我们可能只会丢失⼏个像素。但随着我们应⽤许多连续卷积层，累积丢失的像素数就多了。</p><p>解决这个问题的简单⽅法即为填充（padding）：在输⼊图像的边界填充元素（有零填充、镜像填充、复制填充等）</p><table><thead><tr><th>无填充示意图</th><th>有填充示意图</th></tr></thead><tbody><tr><td><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071810713.gif" alt="无填充示意图"></td><td><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071818918.gif" alt="有填充示意图" style="zoom:70%"></td></tr></tbody></table><h3 id="步幅-Stride">步幅 |Stride</h3><p>在计算互相关时，卷积窗⼝从输⼊张量的左上⻆开始，向下、向右滑动。在前⾯的例⼦中，我们默认每次滑动⼀个元素。</p><p>但是，有时候为了<strong>⾼效计算</strong>或是<strong>缩减采样次数</strong>，卷积窗⼝可以跳过中间位置，每次滑动多个元素。我们将每次滑动元素的数量称为<strong>步幅（stride）</strong>。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071822110.png" alt="步幅示例"></p><div class="note default modern"><p>经过简单的数学推导，我们还可以得出图像经过卷积后的大小：</p><p>新宽度 = (原宽度 - 卷积核宽度 + 2*填充) / 步幅 + 1<br>新高度 = (原高度 - 卷积核高度 + 2*填充) / 步幅 + 1</p></div><h2 id="池化层-汇聚层">池化层/汇聚层</h2><p><strong>池化</strong>（<strong>pooling</strong>）操作一般在卷积层之后，对特征图进行处理。它具有双重⽬的：降低卷积层对位置的敏感性（保持并提取高阶特征），同时降低对空间降采样表⽰的敏感性（防止过拟合）。</p><p>实际上，池化层就是卷积核确定的一种卷积（互相关）操作，也是一种<strong>降采样</strong>操作。</p><p>默认情况下，<strong>深度学习框架中的步幅与池化窗⼝的大小相同</strong>。<br>此外，在处理多通道输⼊数据时，池化层<strong>在每个输入通道上单独运算</strong>，⽽不是像卷积层⼀样在通道上对输⼊进⾏汇总。这意味着汇聚层的输出通道数与输⼊通道数相同。（实际上也是因为它只是做一种特征选择和降采样的工作）</p><p>常见的池化有 最大池化（Max Pooling）和平均池化（Avg Pooling）。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071905248.png" alt="两种池化的示例"></p><h2 id="卷积神经网络-LeNet">卷积神经网络 LeNet</h2><p>LeNet 是最早发布的卷积神经⽹络之⼀，因其在计算机视觉任务中的⾼效性能⽽受到⼴泛关注。<br>这个模型是由 AT&amp;T ⻉尔实验室的研究员Yann LeCun在<strong>1989</strong>年提出的（并以其命名），⽬的是识别图像中的⼿写数字。</p><p>当时，LeNet取得了与⽀持向量机性能相媲美的成果，成为监督学习的主流⽅法。</p><h3 id="网络结构详细">网络结构详细</h3><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310071914369.png" alt="LeNet"></p><h4 id="输入层-Input-layer">输入层|Input layer</h4><p>输入层接收大小为 32×32 的手写数字图像，其中包括灰度值（0-255）。</p><p>在实际应用中，我们通常会对输入图像进行预处理，例如对像素值进行归一化，以加快训练速度和提高模型的准确性。</p><h4 id="卷积层C1-Convolutional-layer-C1">卷积层C1|Convolutional layer C1</h4><p>卷积层C1包括6个卷积核，每个卷积核的大小为 5×5 ，步幅为1，填充为0。</p><p>因此，每个卷积核会产生一个大小为 28×28 的特征图（输出通道数为6）。</p><h4 id="采样层S2-Subsampling-layer-S2">采样层S2|Subsampling layer S2</h4><p>采样层S2采用最大池化（max-pooling）操作，每个窗口的大小为 2×2 ，步幅为2。</p><p>因此，每个池化操作会从4个相邻的特征图中选择最大值，产生一个大小为 14×14 的特征图（输出通道数为6）。这样可以减少特征图的大小，提高计算效率，并且对于轻微的位置变化可以保持一定的不变性。</p><h4 id="卷积层C3-Convolutional-layer-C3">卷积层C3|Convolutional layer C3</h4><p>卷积层C3包括16个卷积核，每个卷积核的大小为 5×5 ，步幅为1，填充为0。</p><p>因此，每个卷积核会产生一个大小为 10×10 的特征图（输出通道数为16）。</p><h4 id="采样层S4-Subsampling-layer-S4">采样层S4|Subsampling layer S4</h4><p>采样层S4采用最大池化操作，每个窗口的大小为 2×2 ，步幅为2。</p><p>因此，每个池化操作会从4个相邻的特征图中选择最大值，产生一个大小为 5×5 的特征图（输出通道数为16）。</p><h4 id="全连接层C5-Fully-connected-layer-C5">全连接层C5|Fully connected layer C5</h4><p>C5将每个大小为 5×5 的特征图拉成一个长度为400的向量，并通过一个带有120个神经元的全连接层进行连接。</p><p>120是由LeNet-5的设计者根据实验得到的最佳值。</p><h4 id="全连接层F6-Fully-connected-layer-F6">全连接层F6|Fully connected layer F6</h4><p>全连接层F6将120个神经元连接到84个神经元。</p><h4 id="输出层-Output-layer">输出层|Output layer</h4><p>输出层由10个神经元组成，每个神经元对应0-9中的一个数字，并输出最终的分类结果。</p><h3 id="PyTorch实现">PyTorch实现</h3><h4 id="引入必要依赖">引入必要依赖</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>): </span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="定义-LeNet-5-模型">定义 LeNet-5 模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义LeNet-5模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool1(torch.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(torch.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="训练模型">训练模型</h4><p>在训练过程中，使用<strong>交叉熵损失函数</strong>计算输出层的误差，并通过反向传播算法更新卷积核和全连接层的权重参数。</p><p><strong>当而，在实际应用中，通常会对LeNet-5进行一些改进，例如增加网络深度、增加卷积核数量、添加正则化等方法，以进一步提高模型的准确性和泛化能力。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载MNIST数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_mnist_data</span>(<span class="params">batch_size = <span class="number">64</span></span>):</span><br><span class="line">    train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">    test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义数据加载器</span></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">return</span> train_loader,test_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, train_data, num_epochs, criterion, learn_rate, device = <span class="literal">None</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_param</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    model.apply(init_param)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用选定 device 加速</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">        device = <span class="built_in">next</span>(<span class="built_in">iter</span>(model.parameters())).device <span class="comment">#指定使用和参数相同的设备</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置优化器</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(),lr=learn_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data):</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            optimizer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">            loss = criterion(model(X), y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step() <span class="comment">#更新所有的参数</span></span><br><span class="line"></span><br><span class="line">            num_batches = <span class="built_in">len</span>(train_data)</span><br><span class="line">            <span class="keyword">if</span> (i+<span class="number">1</span>) % (num_batches//<span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, num_batches, loss.item()))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="模型测试">模型测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_model</span>(<span class="params">model, test_data, device</span>):</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> test_data:</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            outputs = model(X)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += y.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Test Accuracy: &#123;:.2f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练和测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_data, test_data = load_mnist_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = LeNet5()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">device = try_gpu()</span><br><span class="line"></span><br><span class="line">train_model(model, train_data, <span class="number">5</span>, criterion, <span class="number">0.01</span>, device)</span><br><span class="line">test_model(model, test_data, device)</span><br></pre></td></tr></table></figure><p>Output：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">training on cuda:0</span><br><span class="line"></span><br><span class="line">Epoch [1/5], Step [187/938], Loss: 2.1481</span><br><span class="line">Epoch [1/5], Step [374/938], Loss: 0.6471</span><br><span class="line">Epoch [1/5], Step [561/938], Loss: 0.5093</span><br><span class="line">Epoch [1/5], Step [748/938], Loss: 0.5021</span><br><span class="line">Epoch [1/5], Step [935/938], Loss: 0.3279</span><br><span class="line">Epoch [1/5], Step [938/938], Loss: 0.1223</span><br><span class="line">Epoch [2/5], Step [187/938], Loss: 0.3075</span><br><span class="line">Epoch [2/5], Step [374/938], Loss: 0.3728</span><br><span class="line">Epoch [2/5], Step [561/938], Loss: 0.3427</span><br><span class="line">Epoch [2/5], Step [748/938], Loss: 0.2168</span><br><span class="line">Epoch [2/5], Step [935/938], Loss: 0.2212</span><br><span class="line">Epoch [2/5], Step [938/938], Loss: 0.1953</span><br><span class="line">Epoch [3/5], Step [187/938], Loss: 0.2128</span><br><span class="line">Epoch [3/5], Step [374/938], Loss: 0.1818</span><br><span class="line">Epoch [3/5], Step [561/938], Loss: 0.2097</span><br><span class="line">Epoch [3/5], Step [748/938], Loss: 0.2217</span><br><span class="line">Epoch [3/5], Step [935/938], Loss: 0.1669</span><br><span class="line">Epoch [3/5], Step [938/938], Loss: 0.1489</span><br><span class="line">Epoch [4/5], Step [187/938], Loss: 0.3106</span><br><span class="line">Epoch [4/5], Step [374/938], Loss: 0.3208</span><br><span class="line">Epoch [4/5], Step [561/938], Loss: 0.1300</span><br><span class="line">Epoch [4/5], Step [748/938], Loss: 0.2032</span><br><span class="line">Epoch [4/5], Step [935/938], Loss: 0.1013</span><br><span class="line">Epoch [4/5], Step [938/938], Loss: 0.1105</span><br><span class="line">Epoch [5/5], Step [187/938], Loss: 0.0439</span><br><span class="line">Epoch [5/5], Step [374/938], Loss: 0.2064</span><br><span class="line">Epoch [5/5], Step [561/938], Loss: 0.0960</span><br><span class="line">Epoch [5/5], Step [748/938], Loss: 0.1430</span><br><span class="line">Epoch [5/5], Step [935/938], Loss: 0.1692</span><br><span class="line">Epoch [5/5], Step [938/938], Loss: 0.2758</span><br><span class="line"></span><br><span class="line">Test Accuracy: 94.98%</span><br></pre></td></tr></table></figure><h2 id="深度卷积神经网络-AlexNet">深度卷积神经网络 AlexNet</h2><p>在计算机视觉中，直接将神经⽹络与其他机器学习⽅法进⾏⽐较也许不公平。</p><p>卷积神经⽹络的输⼊是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。<br>但传统机器学习⽅法不会将原始像素作为输⼊，而是根据光学、⼏何学、其他知识以及偶然的发现进行<strong>预处理</strong>，通过标准的<strong>特征提取算法</strong>，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他⼿动调整的流⽔线来输⼊数据，最后将提取的特征送⼊传统<strong>分类器</strong>。（例如线性模型或其它核⽅法）</p><p>而部分研究⼈员想法则与众不同：他们认为<strong>特征本身应该被学习</strong>。</p><p>于是，2012年，AlexNet横空出世。它⾸次证明了学习到的特征可以超越⼿⼯设计的特征。它⼀举打破了计算机视觉研究的现状。AlexNet使⽤了8层卷积神经⽹络，并以很⼤的优势赢得了2012年ImageNet图像识别挑战赛。</p><h3 id="网络结构详细-2">网络结构详细</h3><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081122220.png" alt="AlexNet的网络结构"></p><p>AlexNet 输入为RGB三通道的224 × 224 × 3大小的图像。<br>AlexNet 共包含5 个卷积层（包含3个池化）和 3 个全连接层。其中，每个卷积层都包含卷积核、偏置项、<strong>ReLU激活函数和局部响应归一化（LRN）</strong> 模块。<br>第1、2、5个卷积层后面都跟着一个最大池化层，后三个层为全连接层。最终输出层为softmax，将网络输出转化为概率值，用于预测图像的类别。</p><h3 id="AlexNet的创新点">AlexNet的创新点</h3><p>不难发现，AlexNet的网络组成结构和 LeNet-5 十分相似，其在深度和一些处理上做到了创新，对后续 CNNs 的研究起到了关键作用。</p><h4 id="更深的神经网络结构">更深的神经网络结构</h4><p>AlexNet 是首个真正意义上的深度卷积神经网络，它的深度达到了当时先前神经网络的数倍。通过增加网络深度，AlexNet 能够更好地学习数据集的特征，从而提高了图像分类的精度。</p><h4 id="使用ReLU">使用ReLU</h4><p>AlexNet 在CNN<strong>首次</strong>使用了ReLU这一非线性激活函数（LeNet原始版本用的是Sigmoid，而我们前面的PyTorch实现是改进版本）。它在保持计算速度的同时，有效地解决了梯度消失问题，从而使得训练更加高效。</p><h4 id="提出局部响应归一化（LRN）">提出局部响应归一化（LRN）</h4><p>局部响应归一化（Local Response Normalization，LRN）是AlexNet首次提出的，在卷积层和池化层之间添加的一种归一化操作。在卷积层中，每个卷积核都对应一个特征图，LRN就是对这些特征图进行归一化。</p><p>具体来说，对于每个特征图上的每个位置，计算该位置周围的像素的平方和，然后将当前位置的像素值除以这个和。公式如图所示：</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081139682.png" alt="局部响应归一化"></p><p>LRN本质是<strong>抑制邻近神经元的响应，从而增强了神经元的较大响应</strong>。这种技术在一定程度上能够避免过拟合，并提高网络的泛化能力。</p><blockquote><p>在2015年的论文 <em>Very Deep Convolutional Networks for Large-Scale Image Recognition.</em> 中提到LRN基本没什么用。<br>于是在后面的 Googlenet，以及之后的一些CNN架构模型，LRN已经不再使用，因为出现了更加有说服能力的块归一化，也称之为<strong>批量归一化</strong>，即<strong>BN</strong>。</p></blockquote><h4 id="数据增强和Dropout">数据增强和Dropout</h4><p>为了防止过拟合，AlexNet 引入了数据增强和 Dropout 技术。</p><p>数据增强可以通过对图像进行旋转、翻转、裁剪等变换，增加训练数据的多样性，提高模型的泛化能力。</p><p>Dropout 也就是暂退法，在训练过程中随机删除一定比例的神经元，强制网络学习多个互不相同的子网络，从而提高网络的泛化能力。</p><p>详见本站文章：</p><div class="tag link"><a class="link-card" title="Deep Neural Net | 多层感知机与神经网络入门" href="https://qslie.top/posts/4f5fd499"><div class="left"><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/iconNet.png"></div><div class="right"><p class="text">Deep Neural Net | 多层感知机与神经网络入门</p><p class="url">https://qslie.top/posts/4f5fd499</p></div></a></div><h4 id="大规模分布式训练">大规模分布式训练</h4><p>AlexNet 也在2012年对运算问题上实现了重⼤突破。</p><p>Alex Krizhevsky和Ilya Sutskever意识到卷积神经⽹络中的计算瓶颈：卷积和矩阵乘法，都是可以在GPU上并⾏的。<br>由于早期GPU显存有限，他们使⽤两个显存为3GB的NVIDIA GTX580 GPU，采⽤了双数据流设计，使得每个GPU只负责存储和计算模型的⼀半参数，实现了快速卷积运算。</p><p>这种大规模 GPU 集群进行分布式训练的方法在后来的深度学习中也得到了广泛的应用。</p><h2 id="VGG网络">VGG网络</h2><p>VGGNet 是牛津大学计算机视觉组（Visual Geometry Group）和谷歌 DeepMind 一起研究出来的深度卷积神经网络，因而冠名为 VGG。VGG是一种被广泛使用的卷积神经网络结构，在2014年的 ImageNet 大规模视觉识别挑战(ILSVRC -2014)中获得了<strong>亚军</strong>（冠军是 GoogLeNet）。</p><h3 id="网络结构">网络结构</h3><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081205651.png" alt="VGGNet"></p><h3 id="实现VGG块">实现VGG块</h3><p>经过LeNet和AlexNet两种网络的学习我们得出，经典卷积神经⽹络的基本组成部分是下⾯的这几道工序：</p><ol><li>带填充以保持分辨率的卷积层；</li><li>⾮线性激活函数，如ReLU；</li><li>池化层，如Max Pooling。</li></ol><p>而从VGGNet的网络图中可以看出，VGG的卷积层是通过一个个块相连的。<br>其中，块内的卷积层结构相同；块与块之间通过最大池化连接。</p><p>⼀个<strong>VGG块</strong>就包含了上述几道工序：由⼀系列卷积层组成，后⾯再加上⽤于空间下采样的最⼤池化层。</p><p>例如 <strong>VGG-16（这也是人们常说的VGG版本）</strong> 中的某一块，由三个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>conv3-256</mtext></mrow><annotation encoding="application/x-tex">\text{conv3-256}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord text"><span class="mord">conv3-256</span></span></span></span></span> 组成。<br>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>conv3-256</mtext></mrow><annotation encoding="application/x-tex">\text{conv3-256}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord text"><span class="mord">conv3-256</span></span></span></span></span> 表示：这是一个卷积层，卷积核尺寸为3，通道数为256。</p><p>于是，我们可以编写函数实现指定块的生成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">	layers = []</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 创建 num_convs 个 3*3 卷积层</span></span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">		layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">								kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">		<span class="comment"># ReLU激活</span></span><br><span class="line">		layers.append(nn.ReLU())</span><br><span class="line">		in_channels = out_channels</span><br><span class="line">		</span><br><span class="line">	<span class="comment"># 最大池化</span></span><br><span class="line">	layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><h3 id="使用小卷积核">使用小卷积核</h3><p>小卷积核是VGG的一个重要特点。<br>VGG没有采用AlexNet中比较大的卷积核尺寸（如7×7），而是通过降低卷积核的大小（3×3），并增加卷积子层数（构成一个VGG块）来<strong>达到同样的性能</strong>（感受野相同）。</p><p>使用小卷积核还有一些好处：</p><ol><li>大幅度减少模型参数数量；</li><li>小卷积核选取小的 <code>stride</code> 可以防止细节信息的丢失；</li><li>多层卷积层（每个卷积层后都有非线性激活函数），增加非线性，提升模型性能。</li></ol><p>此外，我们注意到在VGG网络结构D中，还使用了1×1卷积核，1×1卷积核可以在不改变感受野的情况下，增加模型的非线性。同时，还可以用它来整合各通道的信息，并输出指定通道数。通道数减小即降维，通道数增加即升维。</p><h3 id="其他特点">其他特点</h3><ul><li><strong>池化层都将上一层的卷积层特征缩减一半</strong></li><li><strong>通道数逐块翻倍</strong></li><li><strong>网络深度较深，参数量较大</strong></li></ul><p>关于第一点，实际上是因为 VGG 统一使用了 2×2 的最大池化；<br>关于第二点，通道数的增加，使得更多的信息可以被提取出来；<br>关于第三点，较深的网络层数使得训练得到的模型分类效果变得更好，但是因此产生了较大的参数学习成本。</p><h2 id="含并行连结的网络-GoogLeNet">含并行连结的网络 GoogLeNet</h2><p>在介绍 VGG 时我们提到，在2014年的ImageNet图像识别挑战赛中，拿到冠军的是GoogLeNet。</p><p>VGG继承了 LeNet 以及 AlexNet 的一些框架结构，而GoogLeNet则做了<strong>更加大胆的网络结构尝试</strong>，虽然深度只有22层，但大小却比 AlexNet 和 VGG 小很多。</p><blockquote><p>GoogleNet参数为500万个<br>AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍</p></blockquote><p>而 GoogLeNet 在结构上所做的大胆尝试就是做了<strong>网络并联</strong>。结构图如下：<br><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081457992.png" alt="GoogLeNet"></p><h3 id="Inception块">Inception块</h3><p>在GoogLeNet中，基本的卷积块被称为Inception块。这很可能得名于电影《盗梦空间》（Inception），因为电影中的⼀句话“我们需要⾛得更深”（“We need to go deeper”）。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081300075.png" alt="Inception块的结构"></p><p>如图所示，Inception块由四条并行路径组成。</p><p>前三条路径使⽤窗⼝大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。</p><p>其中，中间的两条路径在输⼊上<strong>执⾏1×1卷积，以减少通道数</strong>，从⽽降低模型的复杂度。第四条路径使⽤ 3×3最⼤汇聚层，然后使⽤1×1卷积来<strong>改变通道数</strong>。</p><p>四条路径都使⽤合适的填充来使输出大小⼀致，便于最后连结在一起构成Inception块的输出。所以，在Inception块中，通常调整的超参数是每层输出通道数。</p><h3 id="全局平均池化">全局平均池化</h3><p>网络最后采用了全局平均池化层（global average pooling layer）来代替一系列全连接层，显著减少了模型所需参数的数量。该想法来自NIN（Network in Network）。</p><p><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081327503.png" alt="全局平均池化"></p><p>如上图所示，传统方法将输出特征图展平并进行后续全连接层处理，而全局平均池化直接对每一个特征图求平均，其结果直接就作为该特征图的特征。</p><p>具体来说，如果要预测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 个类别，在卷积特征提取部分的最后一层卷积层，设计通道数为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> ，也就是会使得卷积后生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 个特征图，然后通过全局平均池化就可以得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1×1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">1</span></span></span></span> 的特征图，这些<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1×1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7278em;vertical-align:-.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">1</span></span></span></span> 的特征图完全就可以看成是大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 的一维张量，直接将其输入到 Softmax Layer之后，就能直接得到分类结果。</p><div class="note default modern"><p>在 PyTorch 中 可以使用 <code>torch.nn.AdaptiveAvgPool2d(output_size)</code> 并设置参数 <code>outpout_size=1</code> 或 <code>outpout_size=(1,1)</code> 实现。</p></div><h2 id="其他网络……">其他网络……</h2><p>卷积神经网络现今已经得到了蓬勃发展，除了以上提到的经典网络结构以外，还有更多更新的网络在不断更新着。比如残差网络、稠密连接网络，空间金字塔池化方法等等。期待读者自己去探索与发现，本文章暂时就到此告一段落了。</p><h2 id="参考资料">参考资料</h2><ol><li><a target="_blank" rel="noopener" href="https://discuss.d2l.ai/">动手学习深度学习|D2L Discussion - Dive into Deep Learning</a></li><li><a target="_blank" rel="noopener" href="https://liam.page/2017/07/27/convolutions-and-convolution-neural-network/">谈谈离散卷积和卷积神经网络 | 始终 (liam.page)</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/616996325">卷积神经网络经典回顾之LeNet-5 - 知乎</a></li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/88745034">深度学习饱受争议的局部响应归一化(LRN)详解-CSDN博客</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/618545757">卷积神经网络经典回顾之AlexNet - 知乎</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/473446643">手撕 CNN 经典网络之 VGGNet - 知乎</a></li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41990294/article/details/128930017">全局平均池化Global Average Pooling-CSDN博客</a></li></ol></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>图像浅学：卷积神经网络及其发展 - CNN</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://qslie.top/posts/a5df071/">https://qslie.top/posts/a5df071/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Slie</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-10-07</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2024-03-11</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div><div class="post_share"><div class="social-share" data-image="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081327503.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload='this.media="all"'><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png"></a><div class="post-qr-code-desc"></div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png"></a><div class="post-qr-code-desc"></div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.16/audio/coin.mp3"></audio><script defer src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/5aa67a90/" title="恒等映射，残差神经网络的核心思想"><img class="cover" src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081605703.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">恒等映射，残差神经网络的核心思想</div></div></a></div><div class="next-post pull-right"><a href="/posts/9f90e327/" title="Gstp2Vec - 基于图表示方法的出行活动识别"><img class="cover" src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081839733.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Gstp2Vec - 基于图表示方法的出行活动识别</div></div></a></div></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2022/04/09/Likong.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">Slie</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">182</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">98</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/sliefamily"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=1hPHFQqPhpkbBVYbFjRC4q8YQ-A72EFL&amp;noverify=0" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://github.com/SlieFamily" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:sliewdyinwhite@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">秘密基地被发现啦！QAQ</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.</span> <span class="toc-text">为什么是卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E-FC-%E5%88%B0-Conv2d"><span class="toc-number">1.1.</span> <span class="toc-text">从 FC 到 Conv2d</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.1.1.</span> <span class="toc-text">平移不变性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%80%A7"><span class="toc-number">1.1.2.</span> <span class="toc-text">局部性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%92%E7%9B%B8%E5%85%B3%E8%BF%90%E7%AE%97"><span class="toc-number">1.2.</span> <span class="toc-text">互相关运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%9C%AF%E8%AF%AD"><span class="toc-number">2.</span> <span class="toc-text">其他术语</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E9%81%93-Channel"><span class="toc-number">2.1.</span> <span class="toc-text">通道 |Channel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E-Receptive-field"><span class="toc-number">2.2.</span> <span class="toc-text">感受野 |Receptive field</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85-Padding"><span class="toc-number">2.3.</span> <span class="toc-text">填充 |Padding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85-Stride"><span class="toc-number">2.4.</span> <span class="toc-text">步幅 |Stride</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82-%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">3.</span> <span class="toc-text">池化层&#x2F;汇聚层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-LeNet"><span class="toc-number">4.</span> <span class="toc-text">卷积神经网络 LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AF%A6%E7%BB%86"><span class="toc-number">4.1.</span> <span class="toc-text">网络结构详细</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-Input-layer"><span class="toc-number">4.1.1.</span> <span class="toc-text">输入层|Input layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82C1-Convolutional-layer-C1"><span class="toc-number">4.1.2.</span> <span class="toc-text">卷积层C1|Convolutional layer C1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%B1%82S2-Subsampling-layer-S2"><span class="toc-number">4.1.3.</span> <span class="toc-text">采样层S2|Subsampling layer S2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82C3-Convolutional-layer-C3"><span class="toc-number">4.1.4.</span> <span class="toc-text">卷积层C3|Convolutional layer C3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%B1%82S4-Subsampling-layer-S4"><span class="toc-number">4.1.5.</span> <span class="toc-text">采样层S4|Subsampling layer S4</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82C5-Fully-connected-layer-C5"><span class="toc-number">4.1.6.</span> <span class="toc-text">全连接层C5|Fully connected layer C5</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82F6-Fully-connected-layer-F6"><span class="toc-number">4.1.7.</span> <span class="toc-text">全连接层F6|Fully connected layer F6</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82-Output-layer"><span class="toc-number">4.1.8.</span> <span class="toc-text">输出层|Output layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.2.</span> <span class="toc-text">PyTorch实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E5%BF%85%E8%A6%81%E4%BE%9D%E8%B5%96"><span class="toc-number">4.2.1.</span> <span class="toc-text">引入必要依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-LeNet-5-%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.2.</span> <span class="toc-text">定义 LeNet-5 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.3.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-number">4.2.4.</span> <span class="toc-text">模型测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-AlexNet"><span class="toc-number">5.</span> <span class="toc-text">深度卷积神经网络 AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AF%A6%E7%BB%86-2"><span class="toc-number">5.1.</span> <span class="toc-text">网络结构详细</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AlexNet%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">AlexNet的创新点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E6%B7%B1%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.2.1.</span> <span class="toc-text">更深的神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8ReLU"><span class="toc-number">5.2.2.</span> <span class="toc-text">使用ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E5%B1%80%E9%83%A8%E5%93%8D%E5%BA%94%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88LRN%EF%BC%89"><span class="toc-number">5.2.3.</span> <span class="toc-text">提出局部响应归一化（LRN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%92%8CDropout"><span class="toc-number">5.2.4.</span> <span class="toc-text">数据增强和Dropout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">5.2.5.</span> <span class="toc-text">大规模分布式训练</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">VGG网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">6.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0VGG%E5%9D%97"><span class="toc-number">6.2.</span> <span class="toc-text">实现VGG块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B0%8F%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">6.3.</span> <span class="toc-text">使用小卷积核</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%89%B9%E7%82%B9"><span class="toc-number">6.4.</span> <span class="toc-text">其他特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C-GoogLeNet"><span class="toc-number">7.</span> <span class="toc-text">含并行连结的网络 GoogLeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Inception%E5%9D%97"><span class="toc-number">7.1.</span> <span class="toc-text">Inception块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96"><span class="toc-number">7.2.</span> <span class="toc-text">全局平均池化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%BD%91%E7%BB%9C%E2%80%A6%E2%80%A6"><span class="toc-number">8.</span> <span class="toc-text">其他网络……</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">9.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/d3d469f8/" title="Docker - 一步到位的应用部署与上线"><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/docker.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Docker - 一步到位的应用部署与上线"></a><div class="content"><a class="title" href="/posts/d3d469f8/" title="Docker - 一步到位的应用部署与上线">Docker - 一步到位的应用部署与上线</a><time datetime="2024-03-05T07:11:45.000Z" title="发表于 2024-03-05 15:11:45">2024-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c1bb1ae1/" title="GFS - 谷歌的分布式文件存储系统"><img src="http://img.49you.com/20170303/58b8d6f32d651.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="GFS - 谷歌的分布式文件存储系统"></a><div class="content"><a class="title" href="/posts/c1bb1ae1/" title="GFS - 谷歌的分布式文件存储系统">GFS - 谷歌的分布式文件存储系统</a><time datetime="2024-03-05T04:04:07.000Z" title="发表于 2024-03-05 12:04:07">2024-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/f21b6bc8/" title="HuskyLab Servers 0x519 操作指南【自用】"><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/husklab_bg.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="HuskyLab Servers 0x519 操作指南【自用】"></a><div class="content"><a class="title" href="/posts/f21b6bc8/" title="HuskyLab Servers 0x519 操作指南【自用】">HuskyLab Servers 0x519 操作指南【自用】</a><time datetime="2024-03-04T13:29:31.000Z" title="发表于 2024-03-04 21:29:31">2024-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/f7dae821/" title="【快速入门】集群资源管理调度系统 - Slurm"><img src="https://gitee.com/sliewdy/ImageURL-slie/raw/master/imagesNoT/QQ%E5%9B%BE%E7%89%8720210414005713.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【快速入门】集群资源管理调度系统 - Slurm"></a><div class="content"><a class="title" href="/posts/f7dae821/" title="【快速入门】集群资源管理调度系统 - Slurm">【快速入门】集群资源管理调度系统 - Slurm</a><time datetime="2024-03-04T12:50:38.000Z" title="发表于 2024-03-04 20:50:38">2024-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8cb817a9/" title="【最优化】Levenberg-Marquardt 算法"><img src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202401051550627.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【最优化】Levenberg-Marquardt 算法"></a><div class="content"><a class="title" href="/posts/8cb817a9/" title="【最优化】Levenberg-Marquardt 算法">【最优化】Levenberg-Marquardt 算法</a><time datetime="2023-12-22T03:07:39.000Z" title="发表于 2023-12-22 11:07:39">2023-12-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url('https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/202310081327503.png')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Slie</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to Chaos World!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/pluginsSrc/medium-zoom/dist/medium-zoom.min.js"></script><script src="/pluginsSrc/instant.page/instantpage.js" type="module"></script><script src="/pluginsSrc/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("/pluginsSrc/pangu/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><link rel="stylesheet" href="/pluginsSrc/katex/dist/katex.min.css"><script src="/pluginsSrc/katex/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach((a=>{btf.wrap(a,"div",{class:"katex-wrap"})}))</script><script>(()=>{const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";Array.from(e).forEach(((e,n)=>{const d=e.firstElementChild,r="mermaid-"+n,a="%%{init:{ 'theme':'"+t+"'}}%%\n"+d.textContent,i=mermaid.render(r,a);var m;"string"==typeof i?(m=i,d.insertAdjacentHTML("afterend",m)):i.then((({svg:e})=>{d.insertAdjacentHTML("afterend",e)}))}))},n=()=>{window.loadMermaid?t():getScript("/pluginsSrc/mermaid/dist/mermaid.min.js").then(t)};btf.addModeChange("mermaid",t),window.pjax?n():document.addEventListener("DOMContentLoaded",n)})()</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://twikoo.qslie.top/",region:"",onCommentLoaded:function(){btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null)),GLOBAL_CONFIG_SITE.isPost&&(()=>{const t=document.getElementById("twikoo-count");t&&twikoo.getCommentsCount({envId:"https://twikoo.qslie.top/",region:"",urls:[window.location.pathname],includeReply:!1}).then((function(o){t.textContent=o[0].count})).catch((function(t){console.error(t)}))})()},o=()=>{"object"!=typeof twikoo?getScript("/pluginsSrc/twikoo/dist/twikoo.all.min.js").then(t):setTimeout(t,0)};btf.loadComment(document.getElementById("twikoo-wrap"),o)})()</script></div><script async src="/js/diytitle.js"></script><script async src="//at.alicdn.com/t/font_2264842_3izu8i5eoc2.js"></script><script async src="/js/diytitle.js"></script><script data-pjax defer src="/js/fixed_comment.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="/pluginsSrc/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/pluginsSrc/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>if(document.getElementById("recent-posts")&&"/"===location.pathname){var parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://qslie.top/tags/算法/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 算法相关，编程实现 (60)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://qslie.top/tags/故事/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 杂谈日志，光影记录 (6)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://qslie.top/tags/日常/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 记忆碎片，梦境显影 (36)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://qslie.top/tags/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💻 机器学习，数学规划 (35)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://qslie.top/tags" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child)}</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(50% - 5px);background:#f2f2f2;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:#b30070}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style><script data-pjax>function butterfly_swiper_injector_config(){var s=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),s.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/4f5fd499/" alt=""><img width="48" height="48" src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main//Auto/20231030204417.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-11</span><a class="blog-slider__title" href="posts/4f5fd499/" alt="">Deep Neural Net | 多层感知机与神经网络入门</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="posts/4f5fd499/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/9a95977b/" alt=""><img width="48" height="48" src="math-cover.jfif" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-11</span><a class="blog-slider__title" href="posts/9a95977b/" alt="">SLie的&quot;数学基础&quot;集装箱|Perhaps.Ver1.04</a><div class="blog-slider__text">好了，你已经学会1+1=2了，下面来证明一下广义黎曼猜想吧~</div><a class="blog-slider__button" href="posts/9a95977b/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/4ea9ab3c/" alt=""><img width="48" height="48" src="https://jsd.onmicrosoft.cn/gh/SlieFamily/TempImages@main/Auto/202202232045587.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-11</span><a class="blog-slider__title" href="posts/4ea9ab3c/" alt="">算法分析与设计 &amp; 本站算法综述</a><div class="blog-slider__text">本文罗列了常见的经典问题和算法，以及主要的算法分析方法和设计方法</div><a class="blog-slider__button" href="posts/4ea9ab3c/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/5ebaa4d9/" alt=""><img width="48" height="48" src="https://gitee.com/sliewdy/ImageURL-slie/raw/master/imagesNoT/QQ%E5%9B%BE%E7%89%8720210414005713.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-11</span><a class="blog-slider__title" href="posts/5ebaa4d9/" alt="">捣鼓程序时的各种【踩坑】【报错】全收录</a><div class="blog-slider__text">配置环境、安装库包等网上冲浪时遇见到的各式各样奇奇怪怪的Warning和Error，以及各种治标不治本或者又治标又治本的解决方案全记录（个人向）</div><a class="blog-slider__button" href="posts/5ebaa4d9/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="/",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script></body></html>